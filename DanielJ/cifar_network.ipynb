{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding CIFAR Data\n",
    "\n",
    "\n",
    "\n",
    "Now that you understand how two different kinds of networks work for the MNIST dataset, you are going to prepare data for a more general image recognition problem. A lot of the same principles of neural networks apply, but this network is going to be a lot more complicated and robust.\n",
    "\n",
    "For a more general image recognition challenge, you will need a more general dataset. Luckily, there is a dataset called CIFAR that has labeled images for ten different things. Just like the MNIST data, you are going to prepare it to be fed into a network.\n",
    "\n",
    "In this dataset there are 50000 training images and 10000 test images. Each image is 32 x 32 pixels. There are 10 possible output classes. Since these pictures are in color, there are three color channels, one for red, green, and blue.\n",
    "\n",
    "## Preparing the Data\n",
    "\n",
    "\n",
    "This process is very similar to that of the MNIST dataset. See if you can remember what code to add for each step, and click the accordion if you need help. \n",
    "\n",
    "1. Run this cell to import the helper functions and needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper functions\n",
    "def show_min_max(array, i):\n",
    "  random_image = array[i]\n",
    "  print(\"min and max value in image: \", random_image.min(), random_image.max())\n",
    "\n",
    "\n",
    "def plot_image(array, i, labels):\n",
    "  plt.imshow(np.squeeze(array[i]))\n",
    "  plt.title(str(label_names[labels[i]]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add some variables to keep track of image size.  \n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "img_rows = 32\n",
    "img_cols = 32  \n",
    "\n",
    "```\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 32\n",
    "img_cols = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Add a variable to keep track of the number of output classes. \n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "num_classes = 10 \n",
    "\n",
    "```\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Add a variable to keep track of the input shape. \n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "```\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows,img_cols,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Load the data into your program. \n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data() \n",
    "```\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 121s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load a backup copy of the data.  \n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "(train_images_backup, train_labels_backup), (test_images_backup, test_labels_backup) = cifar10.load_data() \n",
    "```\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images_backup, train_labels_backup), (test_images_backup, test_labels_backup) = cifar10.load_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Add the code to set up an array for labels. You can look up the information [here](https://www.cs.toronto.edu/~kriz/cifar.html), or use the dropdown menu.\n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    " label_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] \n",
    "```\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " label_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Add this code to convert the label arrays.\n",
    "\n",
    "```\n",
    " train_labels_backup = [item for sublist in train_labels_backup for item in sublist] \n",
    " test_labels_backup = [item for sublist in test_labels_backup for item in sublist] \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_backup = [item for sublist in train_labels_backup for item in sublist] \n",
    "test_labels_backup = [item for sublist in test_labels_backup for item in sublist] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the shape of the test and train images to make sure you have the correct data. \n",
    "\n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "print(train_images.shape) \n",
    "print(test_images.shape) \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3) (50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape,train_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Use the helper functions to print out the 100th image to ensure the data has loaded correctly.\n",
    "\n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "plot_image(train_images, 100, train_labels_backup)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_images, 100, train_labels_backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Show the min and max values of the 100th image before you normalize the data.\n",
    "\n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "show_min_max(train_images, 100)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min and max value in image:  30 242\n"
     ]
    }
   ],
   "source": [
    "show_min_max(train_images,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Convert the images to the float32 data type.\n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Normalize the data into values between 0 and 1.\n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "plot_image(train_images, 100, train_labels_backup)\n",
    "show_min_max(train_images, 100)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Show the 100th image, and min and max values again to ensure that data preparation has worked correctly.\n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "train_images /= 255\n",
    "test_images /= 255\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Convert the labels using one-hot encoding. \n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Network for CIFAR\n",
    "\n",
    "Now that you have your data prepared it is ready to feed into a network. Since this data is so complicated, it makes sense to use another convolutional network. This kind of network will be slightly less computationally needy and will be able to draw conclusions about the data.  \n",
    "\n",
    "Since you already understand most of the concepts for a convolutional network, this network will allow you to see how those apply to a larger, more general dataset. This network is going to be much more robust and therefore take more time and computing power to train. \n",
    "\n",
    "1. Run the cell below to import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Decide on the number of epochs, batch size, and set the model type. Just like before, you can adjust these as you test your network.\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "epochs = 15 \n",
    "batch_size = 64\n",
    "model = Sequential() \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "model =  Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Convolutional Layers\n",
    "\n",
    "Now that the model is set up, it's time to start adding layers to your network. The last convolutional network you made had one round of convolutional layers followed by a pooling layer, and then some layers to wrap up the network and output a decision.\n",
    "\n",
    "This network is going to need more rounds of layers to figure out information about the pictures. As the layers at first start to decipher what is in the image, the network will start to understand what parts of the image it needs to focus on to understand what it is. This is what makes convolutional neural networks so effective for image recognition. \n",
    "\n",
    "Follow these steps to start to build out the layers of this network. \n",
    "\n",
    "1. To start add a convolutional layer with 32, 3x3 pixel filters.\n",
    "2. Add another one, this time with 64, 3x3 pixel filters. \n",
    "3. Add a pooling layer with a pool size of 2x2 to condense the information the network has learned so far.\n",
    "4. Add a dropout layer with a rate of 0.2 so the network doesn't get too dependent on any specific neurons.\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization \n",
    "\n",
    "Batch normalization is a process that helps standardize the data in between layers. In a more complicated neural network, as each layer reshapes the data it can quickly become extremely complicated, and the network can lose sight of what the goal is. Batch normalization helps to keep each layer on the right track and working towards the desired goal. \n",
    "\n",
    "1. Add a `BatchNormalization()` layer to your model. \n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    " model.add(BatchNormalization()) \n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That rounds off the first stage of this network. You have added convolutional layers with filters to help learn about the image, a pooling layer to condense and organize the information that the network has learned, a dropout layer to prevent overfitting, and a batch normalization layer to help keep all the data normalized. Now it's time to add another set of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding More Layers\n",
    "\n",
    "Because this data is so complicated, you are going to add two more rounds of convolutional layers, pooling layers, dropout layers, and batch normalization. This will help the network to do what it needs to do.\n",
    "\n",
    "\n",
    "1. Add a convolutional layer with: \n",
    "    * filters: 64\n",
    "    * kernel size: (3, 3)\n",
    "    * activation: relu\n",
    "2. Add a max pooling layer with \n",
    "    * pool size: (1, 1)\n",
    "3. Add a dropout layer\n",
    "    * rate: 0.3\n",
    "4. Add a batch normalization layer\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(rate=0.3)) \n",
    "model.add(BatchNormalization())\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(rate=0.3)) \n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Stage of Layers\n",
    "\n",
    "Add another set of layers. This one will need two layers with a lot of filters to help extract more information from the image.\n",
    "\n",
    "Below the previous layers, follow the steps to add the last set of layers. \n",
    "\n",
    "1. Add a convolutional layer with: \n",
    "    * filters: 128\n",
    "    * kernel size: (3, 3)\n",
    "    * activation: relu\n",
    "2. Add another convolutional layer with: \n",
    "    * filters: 64\n",
    "    * kernel size: (3, 3)\n",
    "    * activation: relu\n",
    "3. Add a max pooling layer with a smaller pool size:\n",
    "    * pool size: (1, 1)\n",
    "4. Add a dropout layer with:\n",
    "    * rate: 0.3\n",
    "5. Add a batch normalization layer.\n",
    "\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(rate=0.3)) \n",
    "model.add(BatchNormalization())\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(rate=0.3)) \n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Output Layers\n",
    "\n",
    "Now it's time to prepare the data for the output. You will need to add some layers to organize the information that the network has and reshape it into the decision layer.\n",
    "\n",
    "1. Add a flatten layer to start to prepare the network to produce output.\n",
    "2. Add a Dense layer. \n",
    "    * units: 128\n",
    "    * activation: relu\n",
    "3. And another Dense layer for the output.\n",
    "    * units: num_classes\n",
    "    * activation: softmax\n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=num_classes,activation='softmax'))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the next cell to print a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 14, 14, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 12, 12, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10, 10, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 64)          73792     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 8, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 8, 8, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               524416    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 730442 (2.79 MB)\n",
      "Trainable params: 730058 (2.78 MB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and Training the Model\n",
    "\n",
    "Now that you have the network, you can compile and train it. Just like with all the other networks, you can use Keras's built-in functions to do this. First, you will compile the model and decide on a loss function, optimizer, and metrics. Then, you will use the fit function to train the model using the data. Finally, evaluate the model using the evaluate function. \n",
    "\n",
    "1. First, compile the network. \n",
    "    * loss: keras.losses.categorical_crossentropy\n",
    "    * optimizer: 'adam'\n",
    "    * metrics: ['accuracy']\n",
    "    \n",
    "2. Call the fit function to start the training. \n",
    "3. Save the scores. \n",
    "4. Print the accuracy. \n",
    "\n",
    "<details><summary> Click for final code</summary>\n",
    "\n",
    "```\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(test_images, test_labels), shuffle=True) \n",
    "scores = model.evaluate(test_images, test_labels,verbose=0) \n",
    "print('Test accuracy:', scores[1]) \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 1.5079 - accuracy: 0.4668 - val_loss: 1.3235 - val_accuracy: 0.5299\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 1.0604 - accuracy: 0.6240 - val_loss: 1.0857 - val_accuracy: 0.6171\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.8907 - accuracy: 0.6854 - val_loss: 0.9139 - val_accuracy: 0.6824\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 49s 31ms/step - loss: 0.8001 - accuracy: 0.7172 - val_loss: 0.9133 - val_accuracy: 0.6826\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 0.7379 - accuracy: 0.7405 - val_loss: 0.9157 - val_accuracy: 0.7067\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6797 - accuracy: 0.7613 - val_loss: 1.1966 - val_accuracy: 0.6281\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6420 - accuracy: 0.7735 - val_loss: 0.7250 - val_accuracy: 0.7486\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6056 - accuracy: 0.7884 - val_loss: 0.7192 - val_accuracy: 0.7563\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.5618 - accuracy: 0.8039 - val_loss: 0.6701 - val_accuracy: 0.7802\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.5281 - accuracy: 0.8146 - val_loss: 0.7123 - val_accuracy: 0.7673\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4985 - accuracy: 0.8236 - val_loss: 0.7205 - val_accuracy: 0.7684\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4715 - accuracy: 0.8343 - val_loss: 0.6655 - val_accuracy: 0.7820\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4451 - accuracy: 0.8420 - val_loss: 0.6447 - val_accuracy: 0.7916\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4329 - accuracy: 0.8484 - val_loss: 0.7291 - val_accuracy: 0.7697\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4107 - accuracy: 0.8555 - val_loss: 0.6802 - val_accuracy: 0.7865\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 46s 30ms/step - loss: 0.3915 - accuracy: 0.8607 - val_loss: 0.7038 - val_accuracy: 0.7820\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3724 - accuracy: 0.8680 - val_loss: 0.6736 - val_accuracy: 0.7945\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.3565 - accuracy: 0.8736 - val_loss: 0.6453 - val_accuracy: 0.7983\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.3490 - accuracy: 0.8776 - val_loss: 0.7033 - val_accuracy: 0.7896\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 51s 32ms/step - loss: 0.3285 - accuracy: 0.8835 - val_loss: 0.8517 - val_accuracy: 0.7584\n",
      "Test accuracy: 0.758400022983551\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(test_images, test_labels), shuffle=True) \n",
    "scores = model.evaluate(test_images, test_labels,verbose=0) \n",
    "print('Test accuracy:', scores[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Optionally save the model by running the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cifar_model.keras')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CIfarDataPrepStarter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
